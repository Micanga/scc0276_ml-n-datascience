{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CI8fNbys1WIN"
   },
   "source": [
    "# Exercício 05\n",
    "## Planejamento de Experimentos\n",
    "### Alunos:\n",
    "\n",
    "- Fernanda Tostes Marana (4471070)\n",
    "- Matheus Aparecido do Carmo Alves (9791114)\n",
    "- Thais Bianchini (9791010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ae7mLCZI-DjC"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugQ3MbYx99I-"
   },
   "source": [
    "## Objetivos da prática:\n",
    "- Entender modelagem de dados;\n",
    "- Entender o processo de avaliação;\n",
    "- Trabalhar com procedimentos de amostragem;\n",
    "- Trabalhar com várias medidas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6lfIEgb1WIQ"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Funções novas utilizadas no exercício\n",
    "\n",
    "- [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix#sklearn.metrics.confusion_matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GD84PKIi1WIR"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Escolha, entre as opções abaixo, apenas UM dataset para realizar os exercícios.**\n",
    "\n",
    "**Se o dataset escolhido tiver mais de duas classes, transforme ele num problema binário. Isso pode ser feito escolhendo uma classe para representar a classe positiva e o restante a classe negativa.**\n",
    "\n",
    "**Possíveis datasets:**\n",
    "\n",
    "\n",
    "*   **Câncer de mama:** [sklearn.datasets.load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)\n",
    "*   **Wine:** [sklearn.datasets.load_wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Após a análise dos dados e um pré-processamento vem a etapa de modelagem dos experimentos. Essa etapa pode requerer voltar no pré-processamento caso perceba-se que algo precisa ser feito. A modelagem visa determinar as etapas da execução dos experimentos. No nosso cenário, experimento é a utilização de algoritmos de classificação, regressão ou agrupamento. Para tanto, é preciso definir, com ajuda da análise dos dados, o tipo do problema (classificação, regressão, ...), os atributos/features a serem utilizados e o processo de avaliação.\n",
    "\n",
    "Essa prática foca mais no processo de avaliação.\n",
    "Para a avaliação é preciso definir qual a função de custo/erro adequada, e qual o estimador para o desempenho.\n",
    "\n",
    "Utilizaremos medidas de desempenho para classificação binária baseadas na matriz de confusão (TFP, TFN, TVP, TVN).\n",
    "\n",
    "Nas aplicações reais, o cliente dita qual a medida de desempenho deve ser utilizada, e muitas vezes não é uma das clássicas. E como essa medida, em geral, tem um impacto grande no treinamento do algoritmo de classificação, muitas vezes o algoritmo precisa ser adaptado e isso não é uma tarefa fácil.\n",
    "\n",
    "Após a definição do tipo do problema e da medida de avaliação, é preciso definir como será estimado o desempenho final.\n",
    "\n",
    "Esse processo está ligado á escolha do algoritmo de classificação bem como a escolha de alguns hiperparâmetros. Uma abordagem muito comum na área é a utilização do 10-fold Cross-Validation. Esse procedimento pode ser utilizado para estimar o desempenho do classificador final, bem como, na escolha de alguns poucos hiperparâmetros.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DsLbxZc8sDL"
   },
   "source": [
    "### Questão 01.\n",
    "\n",
    "Dada a introdução acima, já definimos que o tipo do problema é classificação. Defina quais os atributos você utilizará, e a medida de avaliação você acha adequada e explique o porquê dessas escolhas. Você também deve fazer nessa questão os pré-processamentos que achar necessário.\n",
    "\n",
    "Lembre-se que o objetivo da classificação é fazer predições para dados não vistos, ou seja, quando o algoritmo for colocado em produção ele classificará corretamente amostras não vistas.\n",
    "\n",
    "**RESPOSTA:**\n",
    "\n",
    "Primeiramente, iremos carregar o conjunto de dados \"*breast cancer*\", cujo os dados podem ser visualizados a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRDthSyd90j4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "0                  0.2654          0.4601                  0.11890    0.0  \n",
       "1                  0.1860          0.2750                  0.08902    0.0  \n",
       "2                  0.2430          0.3613                  0.08758    0.0  \n",
       "3                  0.2575          0.6638                  0.17300    0.0  \n",
       "4                  0.1625          0.2364                  0.07678    0.0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115    0.0  \n",
       "565                0.1628          0.2572                  0.06637    0.0  \n",
       "566                0.1418          0.2218                  0.07820    0.0  \n",
       "567                0.2650          0.4087                  0.12400    0.0  \n",
       "568                0.0000          0.2871                  0.07039    1.0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# 1. Carregando o conjunto de dados\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# 2. Extraindo os cabeçalhos\n",
    "original_headers = list(breast_cancer.feature_names)\n",
    "original_headers.append('class')\n",
    "\n",
    "# 3. Imprimindo o conjunto de dados parcial\n",
    "breast_cancer_df = pd.DataFrame(data= np.column_stack(\\\n",
    "            (np.array(breast_cancer.data),\\\n",
    "             np.array(breast_cancer.target))),\\\n",
    "            columns=original_headers)\n",
    "breast_cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando o dataset, nota-se que todos os dados são númericos e o problema se trata de um problema de classificação binário.\n",
    "\n",
    "Frente a isso, iremos analisar a estratificação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Construindo um gráfico sobre a distribuição das classes\n",
    "plt.bar([0,1],[list(breast_cancer.target).count(0),list(breast_cancer.target).count(1)],\\\n",
    "    color=['r','b'],align='center',tick_label=breast_cancer.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados apresenta uma distribuição desbalanceada dos dados entre as classes, possuindo mais exemplos da classe 1 (\"*benign*\") do que da classe 0 (\"*malignant*\"). Desta forma, é necessário que, durante o processo de treinamento, se utilize uma abordagem estratificada para generalização e validação do conhecimento.\n",
    "\n",
    "Após a definição do método de validação e treino com o conjunto de dados, passaremos a uma análise sobre a relevância dos parâmetros.\n",
    "\n",
    "O conjunto de dados \"*breast cancer*\" possui 30 parâmetros, dos quais alguns possívelmente podem ser descardados sem uma grande perda de informação. Para realizar está análise, aplicaremos o método de extração de componentes principais (PCA) de forma a definir os parâmetros que melhor representam o conjunto. Define-se que, nesta aplicação, aceitaremos uma perda de no máximo 10% da informação.\n",
    "\n",
    "Contudo, antes da aplicação do PCA, será realizada uma normalização dos dados para uma extração mais precisa das componentes relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Inicializando o algoritmo de normalização \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2. Normalizando os dados\n",
    "breast_cancer_norm = scaler.fit_transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizada a normalização, passaremos a aplicação do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de componentes extraídos: 7\n",
      "Variância explicada: [13.30499079  5.7013746   2.82291016  1.98412752  1.65163324  1.20948224\n",
      "  0.67640888]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Inicializando o método PCA para extrair 90% da informação\n",
    "pca = PCA(.9)\n",
    "\n",
    "# 2. Extraindo os dados relevantes\n",
    "breast_cancer_pca = pca.fit_transform(np.array(breast_cancer_norm))\n",
    "\n",
    "# 3. Imprimindo os resultados\n",
    "print('Número de componentes extraídos:', pca.n_components_)\n",
    "print('Variância explicada:', pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando os resultados, temos que,com a normalização e análise de componentes principais do conjunto de dados, é possível resolver o problema de classificação proposto utilizando 7 dos 30 parâmetros disponíveis, mantendo ainda 90% da informação original.\n",
    "\n",
    "Acredita-se que esta redução de parâmetros e normalização contruibuirá para o aprendizado, uma vez que reduzirá a ambiguidade de certos exemplos e favorecerá a generalização do conhecimento, ao invés da memorização de exemplos.\n",
    "\n",
    "O conjunto de dados final é definido no passo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o conjunto de dados a ser utilizado nos próximos exercícios\n",
    "X, y = breast_cancer_pca, breast_cancer.target\n",
    "headers = [original_headers[list(component).index(max(component))] for component in pca.components_]\n",
    "headers.append('class')\n",
    "data_df = pd.DataFrame(data= np.column_stack((np.array(X),np.array(y))),columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6f0yT8l86PD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Questão 02.\n",
    "Uma boa prática é escolher modelos mais simples, dados dois modelos com desempenho similar a escolha do mais simples é indicada pois com isso há algumas garantias de melhor generalização. Generalização, de maneira geral, é a propriedade que garante que o classificador terá desempenho parecido ao reportado no teste.\n",
    "\n",
    "A definição da complexidade de um modelo nem sempre é uma tarefa fácil. Uma maneira de tentar mensurar isso é através do número de parâmetros do modelo, do tipo de função que ele implementa (linear ou não linear, cortes ortogonais no espaço, ...), ou da chamada dimensão VC (Vapnik-Chervonenkis) do classificador. A dimensão VC é um tópico mais avançado e faz parte da chamada teoria do aprendizado estatístico, ela é citada aqui apenas como curiosidade não é esperado que saibem sobre isso.\n",
    "\n",
    "Execute a função *classificacao* definida no notebook com a medida de desempenho que você definiu. Diga qual o modelo tem o melhor desempenho e explique porque você acha isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaR7HyvO9zss"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def classificacao(data, columns, target, score=balanced_accuracy_score, score_name='acurácia', \n",
    "                  folds=5, plot=True):\n",
    "    \"\"\"\n",
    "    Executa classificação do conjunto de dados passado\n",
    "    ---------------------------------------------------------------\n",
    "    data:       DataFrame. Conjunto de dados\n",
    "    columns:    Lista de inteiros. Índice das colunas utilizadas no treinamento e teste\n",
    "    target:     Inteiro. Índice da coluna alvo\n",
    "    score:      Função. A função que calcula a medida de desempenho desejada. Deve ser uma \n",
    "                função que compara dois vetores, o primeiro vetor são os valores preditos\n",
    "                pelo classificador, o segundo os rótulos reais\n",
    "                Vide exemplo das funções em \n",
    "                http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "                como por exemplo, sklearn.metrics.accuracy_score\n",
    "                http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    score_name: String. Uma string com o nome da medida de desempenho\n",
    "    folds:      Inteiro. Número de folds na validação cruzada\n",
    "    plot:       Booleano. True para plotar os gráficos False para não plotar\n",
    "    ---------------------------------------------------------------\n",
    "    Realiza a classificação em 6 modelos (perceptron, \n",
    "    SVM com kernel polinomial de grau 3, Árvore de decisão, 3NN, 5NN, e 7NN)\n",
    "    Plot o gráfico de desempenho para cada classificador.\n",
    "    Retorna um dicionário com os classificadores treinados, as medidas de desempenho e matriz de confusão\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # inicializa os modelos com os parâmetros solicitados\n",
    "    prcp = Perceptron()\n",
    "    svm_n = SVC(C=10*len(data), kernel='poly', degree=3, gamma=1, coef0=1, cache_size=500, max_iter=1e6)\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=int(len(data)*0.1))\n",
    "    _3nn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto')\n",
    "    _5nn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
    "    _7nn = KNeighborsClassifier(n_neighbors=7, weights='uniform', algorithm='auto')\n",
    "    \n",
    "    clfs = [prcp, svm_n, dt, _3nn, _5nn, _7nn]\n",
    "    clfs_names = ['perceptron', 'svm_poly', 'dt', '3nn', '5nn', '7nn']\n",
    "    \n",
    "    #Inicializa estruturas para matrizes de confusão \n",
    "    confusion_matrices = {\n",
    "        'perceptron':np.array([[0,0],[0,0]]),\n",
    "        'svm_poly':np.array([[0,0],[0,0]]),\n",
    "        'dt':np.array([[0,0],[0,0]]),\n",
    "        '3nn':np.array([[0,0],[0,0]]),\n",
    "        '5nn':np.array([[0,0],[0,0]]),\n",
    "        '7nn':np.array([[0,0],[0,0]])\n",
    "    }\n",
    "\n",
    "    # prepara validação cruzada\n",
    "    # faz divisão do dataset em fold partes\n",
    "    cv = KFold(n_splits=folds, shuffle=True)\n",
    "    \n",
    "    # itera para cada classificador fazendo treino e teste\n",
    "    results = {'perceptron':[], 'svm_poly':[], 'dt':[], '3nn':[], '5nn':[], '7nn':[]}\n",
    "    for c, c_name in zip(clfs, clfs_names):\n",
    "        for train_index, test_index in cv.split(data):\n",
    "            \n",
    "            # separa conjunto de treino e de teste\n",
    "            x_train, y_train = data.iloc[train_index, columns], data.iloc[train_index, target]\n",
    "            x_test, y_test = data.iloc[test_index, columns], data.iloc[test_index, target]\n",
    "            \n",
    "            # faz o treino do modelo\n",
    "            clf = c.fit(X=x_train, y=y_train)\n",
    "            \n",
    "            # valores predito pelo classificador\n",
    "            y_pred = clf.predict(x_test)\n",
    "            # rótulos verdadeiros convertidos para array\n",
    "            y_test = np.array(y_test)\n",
    "            \n",
    "            # realiza predição no conjunto de teste e salva o resultado\n",
    "            results[c_name].append( score(y_test, y_pred) )\n",
    "            confusion_matrices[c_name] += confusion_matrix(y_test, y_pred)        \n",
    "    \n",
    "    if not plot:\n",
    "        return {'results': results, 'clfs':clfs}\n",
    "    # faz o plot de desempenho dos classificadores\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.bar(range(1, len(clfs)+1), [mean(results[name]) for name in clfs_names], \n",
    "                                yerr=[std(results[name]) for name in clfs_names])\n",
    "    plt.xticks(range(1, len(clfs)+1), clfs_names, rotation=45)\n",
    "    title = 'Desempenho dos classificadores - %s'%(score_name)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return {'results': results, 'clfs':clfs, 'confusion_matrices': confusion_matrices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiDHzc6ikoyz"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceptron : 0.9636683616996816 0.02244575957361687\n",
      "svm_poly : 0.9419223663112095 0.010585459648735054\n",
      "dt : 0.9002012339288555 0.0288423613863043\n",
      "3nn : 0.9616188378185282 0.016559009779102508\n",
      "5nn : 0.953419732232704 0.02009249391795451\n",
      "7nn : 0.948619188577102 0.018261942028560935\n"
     ]
    }
   ],
   "source": [
    "result = classificacao(data_df,np.arange(0,len(X[0])),len(X[0]))\n",
    "\n",
    "for cls in result['results']:\n",
    "    print(cls,':',np.mean(result['results'][cls]),np.array(result['results'][cls]).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESPOSTA:**\n",
    "\n",
    "Os classificadores que apresentaram melhores desempenhos foram o *perceptron* e o *3nn*.\n",
    "Não é possível dizer quais dos dois é significativamente melhor que outro, estatisticamente, uma vez que ambos ocupam um intervalo de confiana similar considerando média e desvio padrão.\n",
    "\n",
    "Para confirmar esta afirmação, basta se realizar um teste-p e se analisar a similaridade entre os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njxCEyHt9BH6"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 03.\n",
    "\n",
    "Utilizar os 3 procedimentos de amostragem para estimação do desempenho:\n",
    "- 10-fold Cross Validation;\n",
    "- Leave-one-out;\n",
    "- Boostrap (1000 amostras de boostrap).\n",
    "\n",
    "Para o dataset escolhido, executar os 3 procedimentos acima para estimar o desempenho. Avalie a diferença na variância entre essas abordagens.\n",
    "\n",
    "  * Para o 10-fold Cross Validation e o leave-one-out, você pode utilizar a função *classificacao* já disponível apenas ajustando o parâmetro *folds*.\n",
    "  * Já para o boostrap, você vai precisar implementar a função classificação modificada `classificacao_bootstrap`. A seção de interesse que vocês precisarão modificar está destacada na função. No cálculo do bootstrap utilize 80% do dataset para treino e 20% para teste.\n",
    "\n",
    "Essas execuções podem demorar um pouco, então tenham paciência.\n",
    "\n",
    "**No material complementar há exemplos de como fazer o bootstrap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2wJGs-fDZpw"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-0f9340a90c3a>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-0f9340a90c3a>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    if not plot:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def classificacao_bootstrap(data, columns, target, score=balanced_accuracy_score, no_bs=1000, p_teste=0.2, score_name='acurácia', plot=True):\n",
    "    \"\"\"\n",
    "    Executa classificação do conjunto de dados passado\n",
    "    ---------------------------------------------------------------\n",
    "    data:       DataFrame. Conjunto de dados\n",
    "    columns:    Lista de inteiros. Índice das colunas utilizadas no treinamento e teste\n",
    "    target:     Inteiro. Índice da coluna alvo\n",
    "    score:      Função. A função que calcula a medida de desempenho desejada. Deve ser uma \n",
    "                função que compara dois vetores, o primeiro vetor são os valores preditos\n",
    "                pelo classificador, o segundo os rótulos reais\n",
    "                Vide exemplo das funções em \n",
    "                http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "                como por exemplo, sklearn.metrics.accuracy_score\n",
    "                http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    no_bs:      Número de repetições do bootstrap\n",
    "    p_teste:    Porcentagem para teste da amostra do bootstrap\n",
    "    score_name: String. Uma string com o nome da medida de desempenho    \n",
    "    plot:       Booleano. True para plotar os gráficos False para não plotar\n",
    "    ---------------------------------------------------------------\n",
    "    Realiza a classificação em 6 modelos (perceptron, \n",
    "    SVM com kernel polinomial de grau 3, Árvore de decisão, 3NN, 5NN, e 7NN)\n",
    "    Plot o gráfico de desempenho para cada classificador.\n",
    "    Retorna um dicionário com os classificadores treinados, medidas de desempenho e matriz de confusão\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # inicializa os modelos com os parâmetros solicitados\n",
    "    prcp = Perceptron()\n",
    "    svm_n = SVC(C=10*len(data), kernel='poly', degree=3, gamma=1, coef0=1, cache_size=500, max_iter=1e6)\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=int(len(data)*0.1))\n",
    "    _3nn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto')\n",
    "    _5nn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
    "    _7nn = KNeighborsClassifier(n_neighbors=7, weights='uniform', algorithm='auto')\n",
    "    \n",
    "    clfs = [prcp, svm_n, dt, _3nn, _5nn, _7nn]\n",
    "    clfs_names = ['perceptron', 'svm_poly', 'dt', '3nn', '5nn', '7nn']\n",
    "    confusion_matrices = {\n",
    "        'perceptron':np.array([[0,0],[0,0]]),\n",
    "        'svm_poly':np.array([[0,0],[0,0]]),\n",
    "        'dt':np.array([[0,0],[0,0]]),\n",
    "        '3nn':np.array([[0,0],[0,0]]),\n",
    "        '5nn':np.array([[0,0],[0,0]]),\n",
    "        '7nn':np.array([[0,0],[0,0]])\n",
    "    }\n",
    "    \n",
    "    # itera para cada classificador fazendo treino e teste\n",
    "    results = {'perceptron':[], 'svm_poly':[], 'dt':[], '3nn':[], '5nn':[], '7nn':[]}\n",
    "    for c, c_name in zip(clfs, clfs_names):\n",
    "      for i in range(0, no_bs):\n",
    "        ################# IMPLEMENTE ABAIXO O BOOTSTRAP #######################\n",
    "        #    Sua implementação deve fazer o append no vetor results da mesma \n",
    "        #    forma que a implementação original faz.\n",
    "        #    Dentro desse loop:\n",
    "        #     c: variável-objeto que representa o classificador\n",
    "        #     c_name: Nome do classificador\n",
    "        #     results: vetor de resultado\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "        #######################################################################\n",
    "        \n",
    "    if not plot:\n",
    "        return {'results': results, 'clfs':clfs}\n",
    "    # faz o plot de desempenho dos classificadores\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.bar(range(1, len(clfs)+1), [mean(results[name]) for name in clfs_names], \n",
    "                                yerr=[std(results[name]) for name in clfs_names])\n",
    "    plt.xticks(range(1, len(clfs)+1), clfs_names, rotation=45)\n",
    "    title = 'Desempenho dos classificadores - %s'%(score_name)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return {'results': results, 'clfs':clfs, 'confusion_matrices': confusion_matrices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENLXredtknpo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIT-mDF49RUD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Questão 04.\n",
    "\n",
    "Utilizando o 10-fold cross validation, calcule as medidas de avaliação baseadas na matriz de confusão (TFP, TFN, TVN, TVP). Como em meio a tantas medidas de avaliação, comparar os classificadores? Como escolher o melhor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bey7Lbpkkl_-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercício - Planejamento de Experimentos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
